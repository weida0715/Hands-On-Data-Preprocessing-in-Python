{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Data Preprocessing in Python\n",
    "Learn how to effectively prepare data for successful data analytics\n",
    "    \n",
    "    AUTHOR: Dr. Roy Jafari \n",
    "\n",
    "### Chapter 11: Data Cleaning - Level ⅠⅡ \n",
    "#### Excercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1\n",
    "In this exercise, we will be using 'Temperature_data.csv'. This dataset has some missing values. Do the following.\n",
    "\n",
    "    a. After reading the file into a Pandas DataFrame, check if the dataset is level Ⅰ clean and if not clean it. Also, describe the cleanings if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Species</th>\n",
       "      <th>ReadingDateTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Units</th>\n",
       "      <th>Provisional or Ratified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>1/1/2019 0:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>2/1/2019 0:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>3/1/2019 0:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>4/1/2019 0:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>5/1/2019 0:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>27/12/2019 00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>28/12/2019 00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>29/12/2019 00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>30/12/2019 00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>RI2</td>\n",
       "      <td>TMP</td>\n",
       "      <td>31/12/2019 00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>oC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Site Species   ReadingDateTime  Value Units Provisional or Ratified\n",
       "0    RI2     TMP     1/1/2019 0:00    9.0    oC                       P\n",
       "1    RI2     TMP     2/1/2019 0:00    6.0    oC                       P\n",
       "2    RI2     TMP     3/1/2019 0:00    5.0    oC                       P\n",
       "3    RI2     TMP     4/1/2019 0:00    4.0    oC                       P\n",
       "4    RI2     TMP     5/1/2019 0:00    5.0    oC                       P\n",
       "..   ...     ...               ...    ...   ...                     ...\n",
       "360  RI2     TMP  27/12/2019 00:00    8.0    oC                       P\n",
       "361  RI2     TMP  28/12/2019 00:00    9.0    oC                       P\n",
       "362  RI2     TMP  29/12/2019 00:00    8.0    oC                       P\n",
       "363  RI2     TMP  30/12/2019 00:00    7.0    oC                       P\n",
       "364  RI2     TMP  31/12/2019 00:00    8.0    oC                       P\n",
       "\n",
       "[365 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_df = pd.read_csv('Temperature_data.csv')\n",
    "day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Check if the dataset is level Ⅱ clean and if not clean it. Also, describe the cleanings if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c.\tThe dataset has missing values. See how many, and run diagnosis to see what types of missing values they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d. Are there any outliers in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    e.\tHow should we best deal with the missing values if our goal is to draw multiple boxplots that show the central tendency and variation of temperature across the months? Draw the described visualization after dealing with the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2\n",
    "In this exercise, we are going to use the file ‘Iris_wMV.csv’. Iris data includes 50 samples of three types of iris flowers, totaling 150 rows of data. Each flower is described by its sepal and petal length or width. The column PetalLengthCm has some missing values.\n",
    "\n",
    "    a. Confirm that PetalLengthCm has five missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('Iris_wMV.csv')\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Figure out the types of missing values (MCAR, MAR, MNAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c.\tHow would you best deal with the missing values, if your end goal was to draw the following visualization? Comment on all the four different approaches of dealing with missing values in this chapter, citing why the approach would be or wouldn’t be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.\tDraw the preceding figure twice, once after adopting the “keep as is” approach, and once after adopting “imputing with the central tendency of the appropriate iris Species”. Compare the two figures and comment on their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 3\n",
    "In this exercise, we will be using ‘imdb_top_1000.csv’. More information about this dataset maybe found on this link: https://www.kaggle.com/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows. Perform the following steps for this dataset. \n",
    "\n",
    "    a.\tRead the file into movie_df, and list the level Ⅰ data cleaning steps that the dataset needs. Implement the listed items, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poster_Link</th>\n",
       "      <th>Series_Title</th>\n",
       "      <th>Released_Year</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Overview</th>\n",
       "      <th>Meta_score</th>\n",
       "      <th>Director</th>\n",
       "      <th>Star1</th>\n",
       "      <th>Star2</th>\n",
       "      <th>Star3</th>\n",
       "      <th>Star4</th>\n",
       "      <th>No_of_Votes</th>\n",
       "      <th>Gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMDFkYT...</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>A</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins</td>\n",
       "      <td>Morgan Freeman</td>\n",
       "      <td>Bob Gunton</td>\n",
       "      <td>William Sadler</td>\n",
       "      <td>2343110</td>\n",
       "      <td>28,341,469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Poster_Link  \\\n",
       "0  https://m.media-amazon.com/images/M/MV5BMDFkYT...   \n",
       "\n",
       "               Series_Title Released_Year Certificate  Runtime  Genre  \\\n",
       "0  The Shawshank Redemption          1994           A  142 min  Drama   \n",
       "\n",
       "   IMDB_Rating                                           Overview  Meta_score  \\\n",
       "0          9.3  Two imprisoned men bond over a number of years...        80.0   \n",
       "\n",
       "         Director        Star1           Star2       Star3           Star4  \\\n",
       "0  Frank Darabont  Tim Robbins  Morgan Freeman  Bob Gunton  William Sadler   \n",
       "\n",
       "   No_of_Votes       Gross  \n",
       "0      2343110  28,341,469  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df = pd.read_csv('imdb_top_1000.csv')\n",
    "movie_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b.\tWe want to employ a Decision Tree Classificaiton algorithm using the following columns to predict the IMDB_rating: Certificate, Runtime, Genre, and Gross. For this analytics goals, list the level 2 data cleanings that need to be done, and then implement them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c.\tDoes the dataset have issues, regarding missing values? If yes, how best should we deal with them given the listed data analytic goals in b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d.\tUse the following function from sklearn.tree to create a prediction model that can predict IMDB_rating using Certificate, Runtime, Genre and Gross: \n",
    "    \n",
    "*DecisionTreeRegressor(max_depth=5, min_impurity_decrease=0, min_samples_split=20, splitter='random')*\n",
    "\n",
    "    The tuning parameters have been set for you so the DecsionTreeRegressor can perform better. Once the model is trained draw the trained tree and check if the attribute Gross is used for the prediction of IMDB_rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    e.\tRun the following code and then explain what summary_df is.\n",
    "\n",
    "`\n",
    "dt_predicted_IMDB_rating = RegressTree.predict(Xs)\n",
    "mean_predicted_IMDB_rating = np.ones(len(y))*y.mean()\n",
    "summary_df = pd.DataFrame({'Prediction by Decision Tree': dt_predicted_IMDB_rating, 'Prediction by mean': mean_predicted_IMDB_rating, 'Actual IMDB_rating': y})\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    f.\tRun the following code and explain the visualizaiton it creates. What can you learn from the visualization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary_df['Decision Tree Error'] = abs(summary_df['Prediction by Decision Tree']- summary_df['Actual IMDB_rating'])\n",
    "summary_df['Mean Error'] = abs(summary_df['Prediction by mean'] - summary_df['Actual IMDB_rating'])\n",
    "plt.figure(figsize=(2,10))\n",
    "table = summary_df[['Decision Tree Error','Mean Error']]\n",
    "sns.heatmap(table, cmap='Greys')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 4\n",
    "In this exercise, we will be using two CSV files: responses.csv and columns.csv. The two files are used to record the date of a survey conducted in Slovakia. To access the data on Kaggle.com use this link: https://www.kaggle.com/miroslavsabo/young-people-survey. Perform the following items for this data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I enjoy listening to music.</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I prefer.</td>\n",
       "      <td>Slow songs or fast songs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      original                     short\n",
       "0  I enjoy listening to music.                     Music\n",
       "1                    I prefer.  Slow songs or fast songs"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_df = pd.read_csv('columns.csv')\n",
    "column_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Music</th>\n",
       "      <th>Slow songs or fast songs</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Country</th>\n",
       "      <th>Classical music</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Pop</th>\n",
       "      <th>Rock</th>\n",
       "      <th>Metal or Hardrock</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Number of siblings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Left - right handed</th>\n",
       "      <th>Education</th>\n",
       "      <th>Only child</th>\n",
       "      <th>Village - town</th>\n",
       "      <th>House - block of flats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>no</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>no</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Music  Slow songs or fast songs  Dance  Folk  Country  Classical music  \\\n",
       "0    5.0                       3.0    2.0   1.0      2.0              2.0   \n",
       "1    4.0                       4.0    2.0   1.0      1.0              1.0   \n",
       "\n",
       "   Musical  Pop  Rock  Metal or Hardrock  ...   Age  Height  Weight  \\\n",
       "0      1.0  5.0   5.0                1.0  ...  20.0   163.0    48.0   \n",
       "1      2.0  3.0   5.0                4.0  ...  19.0   163.0    58.0   \n",
       "\n",
       "   Number of siblings  Gender  Left - right handed                Education  \\\n",
       "0                 1.0  female         right handed  college/bachelor degree   \n",
       "1                 2.0  female         right handed  college/bachelor degree   \n",
       "\n",
       "   Only child  Village - town  House - block of flats  \n",
       "0          no         village          block of flats  \n",
       "1          no            city          block of flats  \n",
       "\n",
       "[2 rows x 150 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_df = pd.read_csv('responses.csv')\n",
    "response_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a.\tAre there respondents in this survey that are suspected to be outliers based on their age? How many? list them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b.\tAre there respondents in this survey that are suspected to be outliers based on their level of liking for Country and Hardrock music? How many? list them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tc. Are there respondents in this survey that are suspected to be outliers based on their BMI or Education level? How many? list them in a separate data frame. BMI can be calculated using the following formula. \n",
    "    \n",
    "`BMI=Weight/Height^2`\n",
    "\n",
    "    The Weight has to be in kilograms and height in meters for the following formula. In the dataset, Weight is recorded in kilograms but Height is recorded in centimeters and has to be transformed to Meter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d.\tAre there respondents in this survey that are suspected to be outliers based on their BMI and Age? How many? list them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    e.\tAre there respondents in this survey that are suspected to be outliers based on their BMI and Gender? How many? list them in a separate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 5\n",
    "One of the most common approaches for fraud detection is using outlier detection. In this exercise, you will use 'creditcard.csv' from https://www.kaggle.com/mlg-ulb/creditcardfraud  to evaluate the effectiveness of outlier detection for credit card fraud detection. Pay attention that most of the columns in this data source are processed values to uphold data anonymity. Perform the following steps.\n",
    "\n",
    "    a.\tCheck the state of the dataset for missing values and address them if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = pd.read_csv('creditcard.csv')\n",
    "transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b.\tUsing the column Class, which shows if the transaction has been fraudulent or not, find out what percentage of the transactions in the dataset are fraudulent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c.\tUsing data visualization or the appropriate statistical set, and if necessary both, specify which univariate outliers have a relationship with the column Class? In other words, if the values of which column are outliers then we may suspect fraudulent activity? Which statistical test is appropriate here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d.\tFirst, use the K-Means algorithm to group the transactions into 200 clusters by the attributes that were found to have a relationship with the column Class in part c. Then, filter out the members of the clusters with less than 50 transactions. Does any of them contain significantly fraudulent transactions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    e.\tIf there are any clusters with significant fraudulent transactions, perform centroid analysis for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 6\n",
    "In Chapter 5 and Chapter 8 we used ‘WH Report_preprocessed.csv’ which is the preprocessed version of ‘WH Report.csv’. Now that you have learned numerous data preprocessing skills, you will be preprocessing the dataset yourself.\n",
    "\n",
    "    a.\tCheck the status of the dataset for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.read_csv('WH Report.csv')\n",
    "country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b.\tCheck the status of the dataset for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c.\tWe would like to cluster the countries based on their happiness indices over the years. Based on these analytic goals, address the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d.\tBased on the listed goal in c, address the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    e.\tDoes data need any level Ⅰ or level Ⅱ data cleaning, before clustering is possible? If any, prepare the dataset for k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    f.\tPerform K-means clustering to separate the countries into three groups, and do all the possible analytics that one does when clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 7\n",
    "\n",
    "Specify if the following items describe random errors or systematic errors.\n",
    "\n",
    "    a.\tThe data has these types of errors as the thermometer that the lab has purchased can give precise readings to one-thousandth of a degree\n",
    "    b.\tThe data has these types of errors the survey records were gathered by 5 different surveyors who attended 5 rigorous training sessions\n",
    "    c.\tThe data has these types of errors because when asking for salary questions in a survey there were no options such as “I would not like to share”\n",
    "    d.\tThe data has these types of errors because the cameras were tampered with so the rubbery would not be tapped. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 8\n",
    "Study Figure 11.14 one more time, and run the first three Exercises by the flowchart in this figure and note down the path that led to our decisions regarding the missing values. Did we take steps in dealing with missing values that were not listed in this figure or this chapter? Would it be better to have a more complex figure so every possibility would be included, or not? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 9\n",
    "Explain why the following statement is incorrect: A row may have a significant number of MCAR missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
